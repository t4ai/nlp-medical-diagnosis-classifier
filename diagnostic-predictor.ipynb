{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Patient Diagnosis from Natural Language Symptoms\n",
    "## AAI-501 Team 3 Final Project\n",
    "\n",
    "Team 3 Members:  Tyler Foreman, Christi Moncrief, Tewfik Istanbooly, Mayank Bhatt\n",
    "\n",
    "Date:  August 14, 2023\n",
    "\n",
    "GitHub Repository: https://github.com/t4ai/AAI-501-Team3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "import random\n",
    "import statistics\n",
    "from pprint import pprint\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import uniform\n",
    "\n",
    "# Modeling\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, train_test_split, cross_val_score, cross_val_predict\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, ConfusionMatrixDisplay, make_scorer\n",
    "from scipy.stats import randint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis\n",
    " \n",
    " - Load data into dataframe\n",
    " - Generate and review descriptive statistics of the dataset/variables\n",
    " - Plot visualization of data spread for each variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "symptoms_disease_df = pd.read_csv('./Symptom2Disease.csv')\n",
    "symptoms_disease_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot distribution of diagnoses\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "categories = symptoms_disease_df['label'].value_counts().index\n",
    "counts = symptoms_disease_df['label'].value_counts().values\n",
    "plt.bar(categories, counts, width=0.5)\n",
    "\n",
    "# Add labels\n",
    "plt.ylabel('Count',fontsize=14)\n",
    "plt.xlabel('Diagnosis',fontsize=14)\n",
    "plt.xticks(fontsize=10, rotation = 80)\n",
    "plt.yticks(fontsize=10)\n",
    "\n",
    "# Add Title\n",
    "plt.title('Diagnosis Distribution',fontsize=12);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "-  Split data into test/train/validate datasets (80/10/10)\n",
    "-  Vectorize natrual language text\n",
    "    -  Create experimental datasets with different vectorization approaches - Bag of Words, TFIDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract symptom description text to df X (features)\n",
    "X = symptoms_disease_df['text'].copy()\n",
    "X.head(10)\n",
    "\n",
    "# extract diagnosis into df for y (labels)\n",
    "y = symptoms_disease_df['label'].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into train, validate, test\n",
    "X_train, X_test_val, y_train, y_test_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_test_val, y_test_val, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Bag of Words Vectorizor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with count tokenizer to build vocabulary - fit on train data first\n",
    "count_vectorizor = CountVectorizer()\n",
    "\n",
    "# Tokenize training data to create bag of words - fit the vectorizor on the training set only to avoid data leakage\n",
    "X_train_count = count_vectorizor.fit_transform(X_train)\n",
    "X_train_count.shape\n",
    "\n",
    "# Tokenize test and validation data\n",
    "X_val_count = count_vectorizor.transform(X_val)\n",
    "X_test_count = count_vectorizor.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup TF-IDF Vectorizor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit tfidf vectorizor on training count only to avoid data leakage\n",
    "tf_transformer = TfidfTransformer(use_idf=True).fit(X_train_count)\n",
    "\n",
    "# Vectorize training, val, test data\n",
    "X_train_tfidf = tf_transformer.transform(X_train_count)\n",
    "X_val_tfidf = tf_transformer.transform(X_train_count)\n",
    "X_test_tfidf = tf_transformer.transform(X_train_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection\n",
    "-  Identify 2 models to conduct experiements with (ie: NBC and ---)\n",
    "-  For each model:\n",
    "    -  Train the model on each experimental dataset\n",
    "    -  Validate against validation dataset\n",
    "    -  Tune hyperparameters as necessary to optimize performance\n",
    "    -  Repeat until optimized\n",
    "    -  Test against test dataset\n",
    "    -  Measure model performance\n",
    "- Compare model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base-stats",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
