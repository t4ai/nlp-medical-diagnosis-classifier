{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Patient Diagnosis from Natural Language Symptoms\n",
    "## AAI-501 Team 3 Final Project\n",
    "\n",
    "Team 3 Members:  Tyler Foreman, Christi Moncrief, Tewfik Istanbooly, Mayank Bhatt\n",
    "\n",
    "Date:  August 14, 2023\n",
    "\n",
    "GitHub Repository: https://github.com/t4ai/AAI-501-Team3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "import random\n",
    "import statistics\n",
    "import spacy\n",
    "import pickle\n",
    "from pprint import pprint\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import uniform\n",
    "\n",
    "# Modeling\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, train_test_split, cross_val_score, cross_val_predict\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, ConfusionMatrixDisplay, make_scorer\n",
    "from scipy.stats import randint\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis\n",
    " \n",
    " - Load data into dataframe\n",
    " - Generate and review descriptive statistics of the dataset/variables\n",
    " - Plot visualization of data spread for each variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "symptoms_disease_df = pd.read_csv('./Symptom2Disease.csv')\n",
    "symptoms_disease_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot distribution of diagnoses\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "categories = symptoms_disease_df['label'].value_counts().index\n",
    "counts = symptoms_disease_df['label'].value_counts().values\n",
    "plt.bar(categories, counts, width=0.5)\n",
    "\n",
    "# Add labels\n",
    "plt.ylabel('Count',fontsize=14)\n",
    "plt.xlabel('Diagnosis',fontsize=14)\n",
    "plt.xticks(fontsize=10, rotation = 80)\n",
    "plt.yticks(fontsize=10)\n",
    "\n",
    "# Add Title\n",
    "plt.title('Diagnosis Distribution',fontsize=12);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleanup\n",
    "-  Perform routine cleanup on data:\n",
    "    - remove punctuation marks\n",
    "    - convert to lowercase\n",
    "    - remove numbers\n",
    "    - remove whitespace\n",
    "- Lemmatize the text\n",
    "    - Normalize to base words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove punctuation marks\n",
    "punctuation = '!\"#$%&()*+-/:;<=>?@[\\\\]^_`{|}~'\n",
    "symptoms_disease_df['text'] = symptoms_disease_df['text'].apply(lambda x: ''.join(ch for ch in x if ch not in set(punctuation)))\n",
    "\n",
    "# convert text to lowercase\n",
    "symptoms_disease_df['text'] = symptoms_disease_df['text'].str.lower()\n",
    "\n",
    "# remove numbers\n",
    "symptoms_disease_df['text'] = symptoms_disease_df['text'].str.replace(\"[0-9]\", \" \")\n",
    "\n",
    "# remove whitespaces\n",
    "symptoms_disease_df['text'] = symptoms_disease_df['text'].apply(lambda x:' '.join(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatize to normalize words - use only for Embeddings below\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "\n",
    "# function to lemmatize symptoms text\n",
    "def lemmatization(symptoms):\n",
    "    output = []\n",
    "    for i in symptoms:\n",
    "        s = [token.lemma_ for token in nlp(i)]\n",
    "        output.append(' '.join(s))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "-  Split data into test/train/validate datasets (80/10/10)\n",
    "-  Create 3 datasets for experimentation:\n",
    "    1.  Vectorize natrual language text using TFIDF\n",
    "    2.  Create embeddings using Word2Vec (older approach)\n",
    "    3.  Create embeddings using ELMo (Embeddings from Language Models)\n",
    "- For each of the above, ensure no data leakage by separating train/test\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract symptom description text to df X (features)\n",
    "X = symptoms_disease_df['text'].copy()\n",
    "X.head(10)\n",
    "\n",
    "# extract diagnosis into df for y (labels)\n",
    "y = symptoms_disease_df['label'].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Need to encode y into numeric values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into train, validate, test\n",
    "X_train, X_test_val, y_train, y_test_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_test_val, y_test_val, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare TF-IDF Vectorized datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with count vecotrizer to build vocabulary - fit on train data first\n",
    "count_vectorizer = CountVectorizer()\n",
    "\n",
    "# Vectorize training data to create bag of words - fit the vectorizor on the training set only to avoid data leakage\n",
    "X_train_count = count_vectorizer.fit_transform(X_train)\n",
    "X_train_count.shape\n",
    "\n",
    "# Vectorize test and validation data\n",
    "X_val_count = count_vectorizer.transform(X_val)\n",
    "X_test_count = count_vectorizer.transform(X_test)\n",
    "\n",
    "# Fit tfidf vectorizer on training count only to avoid data leakage\n",
    "tf_transformer = TfidfTransformer(use_idf=True).fit(X_train_count)\n",
    "\n",
    "# Vectorize training, val, test data to TFIDF\n",
    "X_train_tfidf = tf_transformer.transform(X_train_count)\n",
    "X_val_tfidf = tf_transformer.transform(X_val_count)\n",
    "X_test_tfidf = tf_transformer.transform(X_test_count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Word2Vec Embeddings datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup ELMo Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre trained ELMo model\n",
    "elmo = hub.Module(\"https://tfhub.dev/google/elmo/3\", trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions for ELMo\n",
    "def elmo_embeddings(x):\n",
    "  embeddings = elmo(x, signature=\"default\", as_dict=True)[\"elmo\"]\n",
    "\n",
    "  with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.tables_initializer())\n",
    "    # return average of ELMo features\n",
    "    return sess.run(tf.reduce_mean(embeddings,1))\n",
    "  \n",
    "def get_elmo_batches(dataset, batch_size):\n",
    "  return [dataset[i:i+batch_size] for i in range(0,len(dataset),batch_size)]\n",
    "\n",
    "def save_embedding(embedding, file_name):\n",
    "  # save elmo_train_new\n",
    "  pickle_out = open(file_name,\"wb\")\n",
    "  pickle.dump(embedding, pickle_out)\n",
    "  pickle_out.close()\n",
    "\n",
    "def load_saved_embedding(file_name):\n",
    "  pickle_in = open(file_name, \"rb\")\n",
    "  return pickle.load(pickle_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy datasets for elmo\n",
    "X_train_elmo = X_train.copy()\n",
    "X_val_elmo = X_val.copy()\n",
    "X_test_elmo = X_test.copy()\n",
    "\n",
    "# Lemmatize the datasets\n",
    "X_train_elmo = lemmatization(X_train_elmo)\n",
    "X_val_elmo = lemmatization(X_val_elmo)\n",
    "X_test_elmo = lemmatization(X_test_elmo)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get batches for each dataset (to not overwhelm compute)\n",
    "elmo_train_list = get_elmo_batches(X_train_elmo, 100)\n",
    "elmo_val_list = get_elmo_batches(X_val_elmo, 100)\n",
    "elmo_test_list = get_elmo_batches(X_test_elmo, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract ELMo embeddings\n",
    "elmo_train = [elmo_embeddings(x) for x in elmo_train_list]\n",
    "elmo_val = [elmo_embeddings(x) for x in elmo_val_list]\n",
    "elmo_test = [elmo_embeddings(x) for x in elmo_test_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elmo_train_new = np.concatenate(elmo_train, axis = 0)\n",
    "elmo_val_new = np.concatenate(elmo_val, axis = 0)\n",
    "elmo_test_new = np.concatenate(elmo_test, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save embeddings \n",
    "save_embedding(elmo_train_new, \"elmo_train_1690610969.592564106.pickle\")\n",
    "save_embedding(elmo_val_new, \"elmo_val_1690610969.592564106.pickle\")\n",
    "save_embedding(elmo_test_new, \"elmo_test_1690610969.592564106.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection\n",
    "-  Identify 2 models to conduct experiements with (ie: NBC and ---)\n",
    "-  For each model:\n",
    "    -  Train the model on each experimental dataset\n",
    "    -  Validate against validation dataset\n",
    "    -  Tune hyperparameters as necessary to optimize performance\n",
    "    -  Repeat until optimized\n",
    "    -  Test against test dataset\n",
    "    -  Measure model performance\n",
    "- Compare model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 3: Train Classifier on ELMo Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize and fit model on ELMo embeddings --- NBC may not work here\n",
    "clf_elmo = Ridge()\n",
    "clf_elmo.fit(elmo_train_new, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base-stats",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
